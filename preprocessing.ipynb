{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1294bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement nvidia-nccl-cu12==2.21.5 (from versions: 0.0.1.dev5)\n",
      "ERROR: No matching distribution found for nvidia-nccl-cu12==2.21.5\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75c64d",
   "metadata": {},
   "source": [
    "## Load REMI Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e5072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer with vocab size 524\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "REMI_TOK_PATH = \"./vocab_remi.pkl\"  # Path to the saved REMI tokenizer\n",
    "\n",
    "with open(REMI_TOK_PATH, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded tokenizer with vocab size {tokenizer.vocab_size}\")\n",
    "\n",
    "REMI_TOKENIZER = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09baa258",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "midicaps_data\\lmd_full\\0\\0a0a2b0e4d3b7bf4c5383ba025c4683e.mid is not a file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Confirm that the tokenizer works\u001b[39;00m\n\u001b[32m      2\u001b[39m test_file = \u001b[33m\"\u001b[39m\u001b[33m./midicaps_data/lmd_full/0/0a0a2b0e4d3b7bf4c5383ba025c4683e.mid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\miditok\\midi_tokenizer.py:3393\u001b[39m, in \u001b[36mMusicTokenizer.__call__\u001b[39m\u001b[34m(self, obj, *args, **kwargs)\u001b[39m\n\u001b[32m   3391\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decode(tokens[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m], *args, **kwargs)\n\u001b[32m   3392\u001b[39m     \u001b[38;5;66;03m# music file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3395\u001b[39m \u001b[38;5;66;03m# Depreciated miditoolkit object\u001b[39;00m\n\u001b[32m   3396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MidiFile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, MidiFile):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\miditok\\midi_tokenizer.py:1427\u001b[39m, in \u001b[36mMusicTokenizer.encode\u001b[39m\u001b[34m(self, score, encode_ids)\u001b[39m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Load the file if a path was given\u001b[39;00m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score, ScoreTick):\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m     score = \u001b[43mScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[38;5;66;03m# Preprocess the music file\u001b[39;00m\n\u001b[32m   1430\u001b[39m score = \u001b[38;5;28mself\u001b[39m.preprocess_score(score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\symusic\\factory.py:482\u001b[39m, in \u001b[36mScoreFactory.__call__\u001b[39m\u001b[34m(self, x, ttype, fmt)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    477\u001b[39m     x: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mstr\u001b[39m | Path | smt.Score = \u001b[32m960\u001b[39m,\n\u001b[32m    478\u001b[39m     ttype: smt.GeneralTimeUnit = \u001b[33m\"\u001b[39m\u001b[33mtick\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    479\u001b[39m     fmt: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    480\u001b[39m ) -> smt.Score:\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mttype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.from_tpq(x, ttype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\symusic\\factory.py:499\u001b[39m, in \u001b[36mScoreFactory.from_file\u001b[39m\u001b[34m(self, path, ttype, fmt)\u001b[39m\n\u001b[32m    497\u001b[39m     path = Path(path)\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.is_file():\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_ := \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__core_classes.dispatch(ttype).from_file(path, fmt)\n",
      "\u001b[31mValueError\u001b[39m: midicaps_data\\lmd_full\\0\\0a0a2b0e4d3b7bf4c5383ba025c4683e.mid is not a file"
     ]
    }
   ],
   "source": [
    "# Confirm that the tokenizer works\n",
    "test_file = \"./midicaps_data/lmd_full/0/0a0a2b0e4d3b7bf4c5383ba025c4683e.mid\"\n",
    "tokenizer(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccfdc5",
   "metadata": {},
   "source": [
    "# GigaMIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64b3d4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['music', 'NOMML', 'num_tracks', 'TPQN', 'total_notes', 'avg_note_duration', 'avg_velocity', 'min_velocity', 'max_velocity', 'tempo', 'loop_track_idx', 'loop_instrument_type', 'loop_start', 'loop_end', 'loop_duration_beats', 'loop_note_density', 'NOMML_Instrument', 'Type', 'instrument_category: drums-only: 0, all-instruments-with-drums: 1,no-drums: 2', 'md5', 'music_styles_curated', 'music_style_scraped', 'music_style_audio_text_Discogs', 'music_style_audio_text_Lastfm', 'music_style_audio_text_Tagtraum', 'title', 'artist', 'audio_text_matches_score', 'audio_text_matches_sid', 'audio_text_matches_mbid', 'MIDI program number (expressive)', 'instrument_group (expressive)', 'start_tick (expressive)', 'end_tick (expressive)', 'duration_beats (expressive)', 'note_density (expressive)', 'loopability (expressive)'],\n",
       "    num_rows: 1708973\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gigamidi2 = load_dataset(\"Metacreation/GigaMIDI\", \"v2.0.0\", split=\"train\")\n",
    "gigamidi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50353090",
   "metadata": {},
   "source": [
    "### Sample Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8b63ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['music', 'NOMML', 'num_tracks', 'TPQN', 'total_notes', 'avg_note_duration', 'avg_velocity', 'min_velocity', 'max_velocity', 'tempo', 'loop_track_idx', 'loop_instrument_type', 'loop_start', 'loop_end', 'loop_duration_beats', 'loop_note_density', 'NOMML_Instrument', 'Type', 'instrument_category: drums-only: 0, all-instruments-with-drums: 1,no-drums: 2', 'md5', 'music_styles_curated', 'music_style_scraped', 'music_style_audio_text_Discogs', 'music_style_audio_text_Lastfm', 'music_style_audio_text_Tagtraum', 'title', 'artist', 'audio_text_matches_score', 'audio_text_matches_sid', 'audio_text_matches_mbid', 'MIDI program number (expressive)', 'instrument_group (expressive)', 'start_tick (expressive)', 'end_tick (expressive)', 'duration_beats (expressive)', 'note_density (expressive)', 'loopability (expressive)'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "# Generate random indices\n",
    "random_indices = random.sample(range(len(gigamidi2)), num_samples)\n",
    "\n",
    "# Select the random samples\n",
    "sample = gigamidi2.select(random_indices)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087cf78",
   "metadata": {},
   "source": [
    "## Filter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4dbbc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from miditok.constants import SCORE_LOADING_EXCEPTION\n",
    "from miditok.utils import get_bars_ticks\n",
    "from symusic import Score\n",
    "\n",
    "def is_long_enough(\n",
    "    midi_bytes, min_num_bars, min_num_notes\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if ``midi_bytes`` contains the minimum required number of notes and\n",
    "    bars.\n",
    "\n",
    "    :param score: ``symusic.Score`` to inspect or path to a MIDI file.\n",
    "    :param min_num_bars: minimum number of bars the score should contain.\n",
    "    :param min_num_notes: minimum number of notes that score should contain.\n",
    "    :return: boolean indicating if ``score`` is valid.\n",
    "    \"\"\"\n",
    "    # Load score from bytes\n",
    "    try:\n",
    "        score = Score.from_midi(midi_bytes)\n",
    "    except SCORE_LOADING_EXCEPTION:\n",
    "        return False\n",
    "\n",
    "    # reject files with no time signatures\n",
    "    if len(score.time_signatures) == 0:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        bars = get_bars_ticks(score)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return len(bars) >= min_num_bars and score.note_num() > min_num_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db46a89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1708973\n",
      "Dataset size after filtering drum-only tracks: 1028729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1028729/1028729 [03:52<00:00, 4419.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after filtering short files: 420314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = gigamidi2\n",
    "# ds = sample\n",
    "print(f\"Original dataset size: {len(ds)}\")\n",
    "\n",
    "# Filter out drum-only tracks\n",
    "filter_category = 'instrument_category: drums-only: 0, all-instruments-with-drums: 1,no-drums: 2'\n",
    "ds = ds.filter(lambda x: x[filter_category] == 0)\n",
    "\n",
    "print(f\"Dataset size after filtering drum-only tracks: {len(ds)}\")\n",
    "\n",
    "# Filter midi files with less than 8 bars and 32 notes\n",
    "min_num_bars = 8\n",
    "min_num_notes = 32\n",
    "ds = ds.filter(\n",
    "    lambda x: is_long_enough(x[\"music\"], min_num_bars, min_num_notes)\n",
    ")\n",
    "\n",
    "print(f\"Dataset size after filtering short files: {len(ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9e26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['music', 'NOMML', 'num_tracks', 'TPQN', 'total_notes', 'avg_note_duration', 'avg_velocity', 'min_velocity', 'max_velocity', 'tempo', 'loop_track_idx', 'loop_instrument_type', 'loop_start', 'loop_end', 'loop_duration_beats', 'loop_note_density', 'NOMML_Instrument', 'Type', 'instrument_category: drums-only: 0, all-instruments-with-drums: 1,no-drums: 2', 'md5', 'music_styles_curated', 'music_style_scraped', 'music_style_audio_text_Discogs', 'music_style_audio_text_Lastfm', 'music_style_audio_text_Tagtraum', 'title', 'artist', 'audio_text_matches_score', 'audio_text_matches_sid', 'audio_text_matches_mbid', 'MIDI program number (expressive)', 'instrument_group (expressive)', 'start_tick (expressive)', 'end_tick (expressive)', 'duration_beats (expressive)', 'note_density (expressive)', 'loopability (expressive)'],\n",
       "    num_rows: 420314\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['music', 'NOMML', 'num_tracks', 'TPQN', 'total_notes', 'avg_note_duration', 'avg_velocity', 'min_velocity', 'max_velocity', 'tempo', 'loop_track_idx', 'loop_instrument_type', 'loop_start', 'loop_end', 'loop_duration_beats', 'loop_note_density', 'NOMML_Instrument', 'Type', 'instrument_category: drums-only: 0, all-instruments-with-drums: 1,no-drums: 2', 'md5', 'music_styles_curated', 'music_style_scraped', 'music_style_audio_text_Discogs', 'music_style_audio_text_Lastfm', 'music_style_audio_text_Tagtraum', 'title', 'artist', 'audio_text_matches_score', 'audio_text_matches_sid', 'audio_text_matches_mbid', 'MIDI program number (expressive)', 'instrument_group (expressive)', 'start_tick (expressive)', 'end_tick (expressive)', 'duration_beats (expressive)', 'note_density (expressive)', 'loopability (expressive)'],\n",
    "#     num_rows: 420314\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee1679c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(sample[0]['music'], bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b83e51",
   "metadata": {},
   "source": [
    "## Pretokenize GigaMIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8b3e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import tempfile\n",
    "\n",
    "def process_item(args):\n",
    "    (\n",
    "        item,\n",
    "        out_folder,\n",
    "        remi_tokenizer,\n",
    "        max_decoder_len,\n",
    "    ) = args\n",
    "\n",
    "    file_id = item[\"md5\"]                # unique id for output naming\n",
    "    out_path = os.path.join(out_folder, file_id + \".pt\")\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(out_path):\n",
    "        return None\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    # ---- Write temporary MIDI ----\n",
    "    midi_bytes = item[\"music\"]            # HF stores as bytes/string\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mid\", delete=False) as tmp:\n",
    "        tmp.write(midi_bytes)\n",
    "        tmp_path = tmp.name\n",
    "\n",
    "    # ---- Tokenize MIDI ----\n",
    "    midi = None\n",
    "    try:\n",
    "        tokens = remi_tokenizer(tmp_path)\n",
    "\n",
    "        if len(tokens.ids) == 0:\n",
    "            # empty tokenization → treat as skip or BOS/EOS?\n",
    "            # Here we skip because the file probably failed parsing\n",
    "            print(f\"[WARN] Empty token sequence for item {base}, skipping.\")\n",
    "            return \"SKIP\"\n",
    "\n",
    "        midi = (\n",
    "            [remi_tokenizer[\"BOS_None\"]]\n",
    "            + tokens.ids\n",
    "            + [remi_tokenizer[\"EOS_None\"]]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Tokenization failed for item {base}: {e}\")\n",
    "        return \"SKIP\"\n",
    "\n",
    "    finally:\n",
    "        # always clean up temp file\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "\n",
    "    # ---- Pad/truncate ----\n",
    "    if len(midi) < max_decoder_len:\n",
    "        labels = torch.nn.functional.pad(\n",
    "            torch.tensor(midi),\n",
    "            (0, max_decoder_len - len(midi))\n",
    "        )\n",
    "    else:\n",
    "        labels = torch.tensor(midi[:max_decoder_len])\n",
    "\n",
    "    labels = labels.long()\n",
    "\n",
    "    # ---- Save ----\n",
    "    torch.save({\"labels\": labels}, out_path)\n",
    "\n",
    "    return None\n",
    "\n",
    "def pretokenize_gigamidi(\n",
    "    hf_dataset,\n",
    "    out_folder,\n",
    "    remi_tokenizer,\n",
    "    max_decoder_len=1024,\n",
    "    num_workers=8\n",
    "):\n",
    "\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    # Prepare args for each HF row\n",
    "    args_list = [\n",
    "        (item, out_folder, remi_tokenizer, max_decoder_len)\n",
    "        for item in hf_dataset\n",
    "    ]\n",
    "\n",
    "    with Pool(num_workers) as p:\n",
    "        list(\n",
    "            tqdm(\n",
    "                p.imap_unordered(process_item, args_list, chunksize=8),\n",
    "                total=len(args_list)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"Pretokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb18e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ds_to_pretok = ds\n",
    "ds_to_pretok = sample\n",
    "\n",
    "pretokenize_gigamidi(\n",
    "    hf_dataset=ds_to_pretok,\n",
    "    out_folder=\"gigamidi_pretokenized\",\n",
    "    remi_tokenizer=REMI_TOKENIZER,\n",
    "    max_decoder_len=1024,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a3e12",
   "metadata": {},
   "source": [
    "# MIDICaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628d646",
   "metadata": {},
   "source": [
    "## Download MIDICaps MIDI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\josep\\.cache\\huggingface\\hub\\datasets--amaai-lab--MidiCaps. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: ./midi_caps_data\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# repo_id = \"amaai-lab/MidiCaps\"\n",
    "# filename = \"midicaps.tar.gz\"\n",
    "\n",
    "# # Download the file (cached locally)\n",
    "# path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\")\n",
    "\n",
    "# # Optionally extract it\n",
    "# out_dir = \"./midi_caps_data\"\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "# with tarfile.open(path, \"r:gz\") as tar:\n",
    "#     tar.extractall(path=out_dir)\n",
    "\n",
    "# print(\"Extracted to:\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e3447",
   "metadata": {},
   "source": [
    "## Pretokenize MIDICaps Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdf8b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['location', 'caption', 'genre', 'genre_prob', 'mood', 'mood_prob', 'key', 'time_signature', 'tempo', 'tempo_word', 'duration', 'duration_word', 'chord_summary', 'chord_summary_occurence', 'instrument_summary', 'instrument_numbers_sorted', 'all_chords', 'all_chords_timestamps', 'test_set'],\n",
       "    num_rows: 168385\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "midicaps = load_dataset(\"amaai-lab/MidiCaps\", split=\"train\")\n",
    "midicaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bc099",
   "metadata": {},
   "source": [
    "### Sample Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87a26be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['location', 'caption', 'genre', 'genre_prob', 'mood', 'mood_prob', 'key', 'time_signature', 'tempo', 'tempo_word', 'duration', 'duration_word', 'chord_summary', 'chord_summary_occurence', 'instrument_summary', 'instrument_numbers_sorted', 'all_chords', 'all_chords_timestamps', 'test_set'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "# Generate random indices\n",
    "random_indices = random.sample(range(len(midicaps)), num_samples)\n",
    "\n",
    "# Select the random samples\n",
    "sample = midicaps.select(random_indices)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66703f1",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab21d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import T5Tokenizer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_item(args):\n",
    "    (\n",
    "        item,\n",
    "        midi_folder,\n",
    "        out_folder,\n",
    "        remi_tokenizer,\n",
    "        t5_tokenizer,\n",
    "        max_decoder_len,\n",
    "        max_text_len\n",
    "    ) = args\n",
    "\n",
    "    caption = item[\"caption\"]\n",
    "    midi_loc = item[\"location\"]\n",
    "\n",
    "    midi_path = os.path.join(midi_folder, midi_loc)\n",
    "\n",
    "    # Output path (same name but .pt)\n",
    "    base = os.path.splitext(midi_loc)[0]\n",
    "    out_path = os.path.join(out_folder, base + \".pt\")\n",
    "\n",
    "    # Skip if already pretokenized\n",
    "    if os.path.exists(out_path):\n",
    "        return None\n",
    "\n",
    "    # Create subdirectories (mirrored structure)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    # ---- MIDI → REMI ----\n",
    "    tokens = remi_tokenizer(midi_path)\n",
    "    if len(tokens.ids) == 0:\n",
    "        midi = [\n",
    "            remi_tokenizer[\"BOS_None\"],\n",
    "            remi_tokenizer[\"EOS_None\"],\n",
    "        ]\n",
    "    else:\n",
    "        midi = (\n",
    "            [remi_tokenizer[\"BOS_None\"]]\n",
    "            + tokens.ids\n",
    "            + [remi_tokenizer[\"EOS_None\"]]\n",
    "        )\n",
    "\n",
    "    # Pad/truncate\n",
    "    if len(midi) < max_decoder_len:\n",
    "        labels = torch.nn.functional.pad(\n",
    "            torch.tensor(midi),\n",
    "            (0, max_decoder_len - len(midi))\n",
    "        )\n",
    "    else:\n",
    "        labels = torch.tensor(midi[:max_decoder_len])\n",
    "\n",
    "    labels = labels.long()\n",
    "\n",
    "    # ---- Caption → T5 ----\n",
    "    enc = t5_tokenizer(\n",
    "        caption,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_text_len,\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"][0]\n",
    "    attention_mask = enc[\"attention_mask\"][0]\n",
    "\n",
    "    # ---- Save ----\n",
    "    torch.save(\n",
    "        {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        },\n",
    "        out_path\n",
    "    )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def pretokenize_midicaps(\n",
    "    hf_items,\n",
    "    midi_folder,\n",
    "    out_folder,\n",
    "    remi_tokenizer,\n",
    "    max_decoder_len=1024,\n",
    "    max_text_len=256,\n",
    "    num_workers=8\n",
    "):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "    # Prepare arguments for each process\n",
    "    args_list = [\n",
    "        (\n",
    "            item,\n",
    "            midi_folder,\n",
    "            out_folder,\n",
    "            remi_tokenizer,\n",
    "            t5_tokenizer,\n",
    "            max_decoder_len,\n",
    "            max_text_len\n",
    "        )\n",
    "        for item in hf_items\n",
    "    ]\n",
    "\n",
    "    with Pool(num_workers) as p:\n",
    "        list(\n",
    "            tqdm(\n",
    "                p.imap_unordered(process_item, args_list, chunksize=8),\n",
    "                total=len(args_list)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"Pretokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2f96c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%view` not found.\n"
     ]
    }
   ],
   "source": [
    "%view sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f67b193",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m ds_to_pretok = midicaps\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ds_to_pretok = sample\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mpretokenize_midicaps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_to_pretok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmidi_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmidicaps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmidicaps_pretokenized\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremi_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREMI_TOKENIZER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_decoder_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_text_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mpretokenize_midicaps\u001b[39m\u001b[34m(hf_items, midi_folder, out_folder, remi_tokenizer, max_decoder_len, max_text_len, num_workers)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mpretokenize_midicaps\u001b[39m(\n\u001b[32m     86\u001b[39m     hf_items,\n\u001b[32m     87\u001b[39m     midi_folder,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     num_workers=\u001b[32m8\u001b[39m\n\u001b[32m     93\u001b[39m ):\n\u001b[32m     94\u001b[39m     os.makedirs(out_folder, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     t5_tokenizer = \u001b[43mT5Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mgoogle/flan-t5-base\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# Prepare arguments for each process\u001b[39;00m\n\u001b[32m     99\u001b[39m     args_list = [\n\u001b[32m    100\u001b[39m         (\n\u001b[32m    101\u001b[39m             item,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m hf_items\n\u001b[32m    110\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2157\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\miniforge3\\envs\\csc413\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2143\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2140\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "ds_to_pretok = midicaps\n",
    "# ds_to_pretok = sample\n",
    "\n",
    "pretokenize_midicaps(\n",
    "    hf_items=ds_to_pretok,\n",
    "    midi_folder=\"midicaps\",\n",
    "    out_folder=\"midicaps_pretokenized\",\n",
    "    remi_tokenizer=REMI_TOKENIZER,\n",
    "    max_decoder_len=1024,\n",
    "    max_text_len=256,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d897a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
